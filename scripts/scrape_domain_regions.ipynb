{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "northern-victoria-vic 831 range(1, 42)\n",
      "melbourne-region-vic 11628 range(1, 50)\n",
      "eastern-suburbs-vic 1642 range(1, 50)\n",
      "south-western-victoria-vic 500 range(1, 25)\n",
      "gippsland-greater-region-vic 537 range(1, 27)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "A very simple and basic web scraping script. Feel free to\n",
    "use this as a source of inspiration, but, make sure to attribute\n",
    "it if you do so.\n",
    "\n",
    "This is by no means production code.\n",
    "\"\"\"\n",
    "# built-in imports\n",
    "import re\n",
    "from json import dump\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# user packages\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# constants\n",
    "BASE_URL = \"https://www.domain.com.au\"\n",
    "N_PAGES = range(2, 3) # update this to your liking\n",
    "\n",
    "# begin code\n",
    "url_links = []\n",
    "property_metadata = defaultdict(dict)\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"}\n",
    "# generate list of urls to visit\n",
    "regions = [\"northern-victoria-vic\",\"melbourne-region-vic\",\"eastern-suburbs-vic\",\"south-western-victoria-vic\",\"gippsland-greater-region-vic\"]\n",
    "for region in regions:\n",
    "    url = BASE_URL + f\"/rent/{region}/?sort=price-desc&page=1\"\n",
    "    bs_object = BeautifulSoup(requests.get(url, headers=headers).text, \"html.parser\")\n",
    "    # find the unordered list (ul) elements which are the results, then\n",
    "    # find all href (a) tags that are from the base_url website.\n",
    "    result_count = int(bs_object \\\n",
    "        .find(\n",
    "            \"h1\",\n",
    "            {\"data-testid\": \"summary\",\"class\":\"css-ekkwk0\"}\n",
    "        ).text.split()[0])\n",
    "    if result_count < 1000:\n",
    "        num_pages = range(1,math.ceil(result_count/20))\n",
    "    else:\n",
    "        num_pages = range(1,50)\n",
    "    print(region, result_count,num_pages)\n",
    "    for page in num_pages:\n",
    "        url = BASE_URL + f\"/rent/{region}/?sort=price-desc&page={page}\"\n",
    "        bs_object = BeautifulSoup(requests.get(url, headers=headers).text, \"html.parser\")\n",
    "\n",
    "        # find the unordered list (ul) elements which are the results, then\n",
    "        # find all href (a) tags that are from the base_url website.\n",
    "        index_links = bs_object \\\n",
    "            .find(\n",
    "                \"ul\",\n",
    "                {\"data-testid\": \"results\"}\n",
    "            ) \\\n",
    "            .findAll(\n",
    "                \"a\",\n",
    "                href=re.compile(f\"{BASE_URL}/*\") # the `*` denotes wildcard any\n",
    "            )\n",
    "\n",
    "        for link in index_links:\n",
    "            # if its a property address, add it to the list\n",
    "            if 'address' in link['class']:\n",
    "                url_links.append(link['href'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3780"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(url_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each url, scrape some basic metadata\n",
    "for property_url in url_links[1:]:\n",
    "    bs_object = BeautifulSoup(requests.get(property_url, headers=headers).text, \"html.parser\")\n",
    "    # looks for the header class to get property name\n",
    "    property_metadata[property_url]['name'] = bs_object \\\n",
    "        .find(\"h1\", {\"class\": \"css-164r41r\"}) \\\n",
    "        .text\n",
    "    property_metadata[property_url]['type'] = bs_object \\\n",
    "        .find(\"div\", {\"data-testid\": \"listing-summary-property-type\"}) \\\n",
    "        .text\n",
    "    # looks for the div containing a summary title for cost\n",
    "    property_metadata[property_url]['cost_text'] = bs_object \\\n",
    "        .find(\"div\", {\"data-testid\": \"listing-details__summary-title\"}) \\\n",
    "        .text\n",
    "\n",
    "    # extract coordinates from the hyperlink provided\n",
    "    # i'll let you figure out what this does :P\n",
    "    property_metadata[property_url]['coordinates'] = [\n",
    "        float(coord) for coord in re.findall(\n",
    "            r'destination=([-\\s,\\d\\.]+)', # use regex101.com here if you need to\n",
    "            bs_object \\\n",
    "                .find(\n",
    "                    \"a\",\n",
    "                    {\"target\": \"_blank\", 'rel': \"noopener noreferer\"}\n",
    "                ) \\\n",
    "                .attrs['href']\n",
    "        )[0].split(',')\n",
    "    ]\n",
    "\n",
    "    property_metadata[property_url]['rooms'] = [\n",
    "        re.findall(r'\\d\\s[A-Za-z]+', feature.text) for feature in bs_object \\\n",
    "            .find(\"div\", {\"data-testid\": \"property-features\"}) \\\n",
    "            .findAll(\"span\", {\"data-testid\": \"property-features-text-container\"})\n",
    "    ]\n",
    "\n",
    "\n",
    "# output to example json in data/raw/\n",
    "with open('../data/raw/domain_scrap_region.json', 'w') as f:\n",
    "    dump(property_metadata, f)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "f = open('../data/raw/domain_scrap_region.json')\n",
    "  \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3779"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
